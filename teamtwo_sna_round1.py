# -*- coding: utf-8 -*-
"""TeamTwo_SNA round1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i_mLDCUyzd_PbITDVuelvf0j2Z6Bgo0_

Importing Libraries
"""

import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import math

"""Loading Dataset 1 - Road Network"""

df1 = pd.read_csv('newdata2.csv')
G1 = nx.Graph()
G1 = nx.from_pandas_edgelist(df1, source='Id1', target='Id2')

"""Plotting Degree Distribution function"""

def plot_degree_dist(G):
    degree_hist = nx.degree_histogram(G)
    degree_hist = np.array(degree_hist, dtype=float)
    degree_prob = degree_hist/G.number_of_nodes()
    plt.loglog(np.arange(degree_prob.shape[0]),degree_prob,'b.')
    plt.xlabel('k')
    plt.ylabel('p(k)')
    plt.title('Degree Distribution')
    print(G)
    plt.show()

"""Plotting Degree Distribution frequency function"""

def plot_degree_dist_freq(G):
  degrees = [val for (node, val) in G1.degree()]
  plt.hist(degrees, bins=np.arange(min(degrees)-0.5, max(degrees)+1.5, 1), color='skyblue', edgecolor='black')
  plt.title("Degree Distribution of the Network")
  plt.xlabel("Degree")
  plt.ylabel("Frequency")
  plt.show()

"""Max degree function"""

def max_degree(G):
# Get the degree of each node in the graph
  degrees = [degree for node, degree in G1.degree()]

# Find the maximum degree
  max_degree = max(degrees)

  return max_degree

"""Min degree function"""

def minm_degree(G):
# Get the degree of each node in the graph
  degrees = [degree for node, degree in G1.degree()]

# Find the minimum degree
  min_degree = min(degrees)

  return min_degree

"""Avg degree function"""

def avg_degree(G):
 average_degree = sum(dict(nx.degree(G1)).values()) / len(G1)
 return average_degree

"""Standard deviation function"""

def std(G):
# Assuming G is your NetworkX graph
 degree_sequence = [d for n, d in G.degree()]
 degree_hist = np.histogram(degree_sequence, bins=np.arange(max(degree_sequence)+2))[0]

# Calculate standard deviation
 degree_std_dev = np.std(degree_sequence)
 return degree_std_dev

"""Centrality Measure Function"""

"""Centrality Measure"""

centrality_measures = ['Degree Centrality','Eigenvector Centrality', 'Pagerank','Katz Centrality','Closeness Centrality', 'Betweenness Centrality','Local Clustering Coefficient']

def centrality_val(d,i):

# Sort nodes based on node ID in increasing order
  sorted_nodes = sorted(d.items(), key=lambda x: x[0])

# Create a DataFrame to store node IDs and their corresponding centrality values
  data = {'Node ID': [node[0] for node in sorted_nodes], centrality_measures[i]: [node[1] for node in sorted_nodes]}
  df = pd.DataFrame(data)

# Display the DataFrame
  print(df)

  for node in range(0, 21):
    if node in d:
        print("Node:", node,centrality_measures[i],":", d[node])
    else:
        print("Node:", node, centrality_measures[i],": Node not found")

"""Visualising & Plotting Centrality"""

def visualize_centrality(d,i):
# Lists to store node IDs and degree centrality values
  nodes = []
  centralities = []

# Calculate the number of nodes in the graph
  num_nodes = G1.number_of_nodes()

# Iterate through the  nodes and store their  centrality values
  for node in range(1, num_nodes):
      if node in d:
          nodes.append(node)
          centralities.append(d[node])

# Plotting the centrality values
  plt.figure(figsize=(40, 24))
  plt.plot(nodes, centralities, marker='o', linestyle='-')
  plt.xlabel('Node')
  plt.ylabel(centrality_measures[i])
  plt.title("Centrality of the Nodes")
  plt.grid(True)
  plt.show()

"""Top 10 Centrality"""

def top_10_centrality(d,i):
# Sort nodes based on centrality (highest centrality first)
  sorted_nodes = sorted(d, key=d.get, reverse=True)

# Get top 10 nodes with highest centrality
  top_10_nodes = sorted_nodes[:10]
  top_10_degrees = [d[node] for node in top_10_nodes]

# Create a DataFrame to display the top 10 nodes and their centrality values
  data = {'Node ID': top_10_nodes,centrality_measures[i] : top_10_degrees}
  df = pd.DataFrame(data)

# Display the DataFrame
  print(df)

"""Degree Centrality"""

def degree_centrality(G):

    nodes =[]
# Calculate the number of nodes in the graph
    num_nodes = G1.number_of_nodes()
# Iterate through the  nodes and store their  centrality values
    for node in range(1, num_nodes):
        nodes.append(node)


    top = set(nodes)
    bottom = set(G) - top
    s = 1.0 / len(bottom)
    centrality = dict((n, d * s) for n, d in G.degree(top))
    s = 1.0 / len(top)
    centrality.update(dict((n, d * s) for n, d in G.degree(bottom)))
    num_nodes = G.number_of_nodes()
    centrality = {nodes: centrality[nodes] / num_nodes for nodes in centrality}

    return centrality

"""Plotting G1"""

# Assuming 'G1' is the network graph

# Plotting the Degree Distribution

plot_degree_dist_freq(G1)
plot_degree_dist(G1)

# Finding Max and Min Degree
maxi_degree = max_degree(G1)
mini_degree = minm_degree(G1)

# Calculating Average Degree
average_degree = avg_degree(G1)

# Determining Standard Deviation of Degree Distribution
std_dev_degree = std(G1)

print("Maximum Degree:", maxi_degree)
print("Minimum Degree:", mini_degree)
print("Average Degree:", average_degree)
print("Standard Deviation of Degree Distribution:",std_dev_degree)

"""Degree Centrality for G1"""

d1 = nx.degree_centrality(G1)
#d1= {node: d1[node] for node in range(1, 1001)}
centrality_val(d1,0)
visualize_centrality(d1,0)
top_10_centrality(d1,0)

"""EigenVector Centrality for G1"""

e1 = nx.eigenvector_centrality(G1)
#e1= {node: e1[node] for node in range(1, 1001)}
centrality_val(e1,1)
visualize_centrality(e1,1)
top_10_centrality(e1,1)

"""Katz Centrality for G1"""

from scipy.sparse.linalg import eigs

# Convert the graph to a NumPy matrix with float data type
A = nx.to_numpy_array(G1, dtype=float)

try:
    # Compute the largest eigenvector using power iteration method
    eigenvalues, eigenvectors = eigs(A, k=1, which='LR', maxiter=2000, tol=1e-4)

    # Extract the largest eigenvector
    largest_eigenvector = np.squeeze(np.asarray(eigenvectors))

    # Normalize the largest eigenvector to obtain Katz centrality
    katz_centrality = largest_eigenvector / np.linalg.norm(largest_eigenvector)

    # Convert the result to a dictionary format
    katz_centrality = {node: katz_centrality[i] for i, node in enumerate(G1.nodes())}

    # Print or use the Katz centrality dictionary
    centrality_val(katz_centrality, 3)
    visualize_centrality(katz_centrality, 3)
    top_10_centrality(katz_centrality, 3)

except Exception as e:
    print("Error:", e)

"""Page Rank Centrality for G1"""

try:
    p1 = nx.pagerank(G1)
except nx.PowerIterationFailedConvergence as e:
    print("Power iteration failed to converge:", e)
centrality_val(p1,2)
visualize_centrality(p1,2)
top_10_centrality(p1,2)

"""Closeness Centrality for G1"""

c1 = {}
# Calculate closeness centrality for each node from 1 to 1000
for node in range(1, 1001):
    c1[node] = nx.closeness_centrality(G1, u=node)

centrality_val(c1,4)
visualize_centrality(c1,4)
top_10_centrality(c1,4)

"""Betweeness Centrality for G1"""

try:
    b1 = nx.betweenness_centrality(G1)
except nx.PowerIterationFailedConvergence as e:
    print("Power iteration failed to converge:", e)
centrality_val(b1,5)
visualize_centrality(b1,5)
top_10_centrality(b1,5)

try:
    b2 = nx.betweenness_centrality(G1)
except nx.PowerIterationFailedConvergence as e:
    print("Power iteration failed to converge:", e)
centrality_val(b2,5)
visualize_centrality(b2,5)
top_10_centrality(b2,5)

"""Clustering coefficient G1"""

l1 = nx.clustering(G1)
centrality_val(l1,6)
visualize_centrality(l1,6)
top_10_centrality(l1,6)
#clustering coeff isliye 0 aa rha kyuki there are no triangles being formed in the networks and other few reasons

"""Global & Average coefficient G1"""

g1= nx.average_clustering(G1)
print("\nGlobal Clustering Coefficient of G1:")
print(g1)

a1 = sum(l1.values()) / len(l1)
print("\n Average Clustering Coefficient of G1:")
print(a1)

"""Reciprocity of G1"""

reciprocity1 = nx.reciprocity(G1)
print("\nReciprocity of G1:")
print(reciprocity1)

"""Tranisitivity of G1"""

transitivity1 = nx.transitivity(G1)
print("\nTransitivity of G1:")
print(transitivity1)

"""Loading Dataset 2 - Web Google"""

df2 = pd.read_csv('lastfm_asia_edges.csv')
G2 = nx.Graph()
G2 = nx.from_pandas_edgelist(df2, source='node_1', target='node_2')

"""Plotting G2"""

# Assuming 'G2' is the network graph

# Plotting the Degree Distribution

plot_degree_dist_freq(G2)
plot_degree_dist(G2)

# Finding Max and Min Degree
maxi_degree = max_degree(G2)
mini_degree = minm_degree(G2)

# Calculating Average Degree
average_degree = avg_degree(G2)

# Determining Standard Deviation of Degree Distribution
std_dev_degree = std(G2)

print("Maximum Degree:", maxi_degree)
print("Minimum Degree:", mini_degree)
print("Average Degree:", average_degree)
print("Standard Deviation of Degree Distribution:",std_dev_degree)

"""Degree Centrality for G2"""

d2 = nx.degree_centrality(G2)
#d2= {node: d2[node] for node in range(1, 1001)}
centrality_val(d2,0)
visualize_centrality(d2,0)
top_10_centrality(d2,0)

"""Eigen Vector Centrality for G2"""

e2 = nx.eigenvector_centrality(G2)
#e2= {node: e2[node] for node in range(1, 1001)}
centrality_val(e2,1)
visualize_centrality(e2,1)
top_10_centrality(e2,1)

"""Katz Centrality for G2"""

from scipy.sparse.linalg import eigs

# Convert the graph to a NumPy matrix with float data type
A = nx.to_numpy_array(G2, dtype=float)

try:
    # Compute the largest eigenvector using power iteration method
    eigenvalues, eigenvectors = eigs(A, k=1, which='LR', maxiter=2000, tol=1e-4)

    # Extract the largest eigenvector
    largest_eigenvector = np.squeeze(np.asarray(eigenvectors))

    # Normalize the largest eigenvector to obtain Katz centrality
    katz_centrality = largest_eigenvector / np.linalg.norm(largest_eigenvector)

    # Convert the result to a dictionary format
    katz_centrality = {node: katz_centrality[i] for i, node in enumerate(G2.nodes())}

    # Print or use the Katz centrality dictionary
    centrality_val(katz_centrality, 3)
    visualize_centrality(katz_centrality, 3)
    top_10_centrality(katz_centrality, 3)

except Exception as e:
    print("Error:", e)

"""Page Rank Centrality for G2"""

p2 = nx.pagerank(G2)
centrality_val(p2,2)
visualize_centrality(p2,2)
top_10_centrality(p2,2)

"""Closeness Centrality for G2"""

c2 = {}
# Calculate closeness centrality for each node from 1 to 1000
for node in range(1, 1001):
    c2[node] = nx.closeness_centrality(G2, u=node)
centrality_val(c2,4)
visualize_centrality(c2,4)
top_10_centrality(c2,4)

"""Betweeness Centrality for G2"""

subgraph_nodes = [node for node in G2.nodes() if 1 <= node <= 2000]
G_sub = G2.subgraph(subgraph_nodes)

try:
    b1 = nx.betweenness_centrality(G_sub)
except nx.PowerIterationFailedConvergence as e:
    print("Power iteration failed to converge:", e)

node_to_check = 5
centrality_val(b1, node_to_check)
visualize_centrality(b1, node_to_check)
top_10_centrality(b1, node_to_check)

"""Clustering coefficient G2"""

l2 = nx.clustering(G2)
centrality_val(l2,6)
visualize_centrality(l2,6)
top_10_centrality(l2,6)

"""Global & Average coefficient G2"""

g2= nx.average_clustering(G2)
print("\nGlobal Clustering Coefficient of G2:")
print(g2)

a2 = sum(l2.values()) / len(l2)
print("\n Average Clustering Coefficient of G2")
print(a2)

"""Reciprocity of G2"""

reciprocity2 = nx.reciprocity(G2)
print("\nReciprocity of G2:")
print(reciprocity2)

"""Transitivity of G2"""

transitivity2 = nx.transitivity(G2)
print("\nTransitivity of G2:")
print(transitivity2)

# Suppose you want to copy 100 edges from G1 to G2
G = nx.Graph()
edges_to_copy = list(G1.edges())[:10000]  # Extract the first 100 edges from G1

# Add the selected edges to the second graph (G2)
G.add_edges_from(edges_to_copy)
try:
    centrality = nx.betweenness_centrality(G)
except nx.PowerIterationFailedConvergence as e:
    print("Power iteration failed to converge:", e)
k1=centrality
centrality_val(k1,3)
visualize_centrality(k1,3)
top_10_centrality(k1,3)

df1.to_csv('Biodata.csv', index=False)
# Saving the figure.
plt.savefig("output.jpg")